
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>OnnxRuntime GenAI (OGA) Flow &#8212; Ryzen AI Software 1.5 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=643846e8" />
    <link rel="stylesheet" type="text/css" href="_static/llm-table.css?v=d34acfde" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_header.css?v=9557e3d1" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_footer.css?v=7095035a" />
    <link rel="stylesheet" type="text/css" href="_static/fonts.css?v=fcff5274" />
    <link rel="stylesheet" type="text/css" href="_static/_static/llm-table.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9f1c6d22"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script defer="defer" src="_static/search.js?v=90a4452c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'hybrid_oga';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="canonical" href="ryzenai.docs.amd.com/hybrid_oga.html" />
    <link rel="icon" href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Preparing OGA Models" href="oga_model_prepare.html" />
    <link rel="prev" title="High-Level Python SDK" href="llm/high_level_python.html" />
    <meta name="google-site-verification" content="vo35SZt_GASsTHAEmdww7AYKPCvZyzLvOXBl8guBME4" />

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="/">Ryzen AI</a>
                
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/vgodsoe/ryzen-ai-documentation" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://community.amd.com/t5/ai/ct-p/amd_ai" id="navcommunity" role="button" aria-expanded="false" target="_blank" >
                                Community
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://www.amd.com/en/products/ryzen-ai" id="navproducts" role="button" aria-expanded="false" target="_blank" >
                                Products
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Ryzen AI Software 1.5 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="relnotes.html">Release Notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="inst.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples, Demos, Tutorials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Models on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="model_quantization.html">Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="modelrun.html">Model Compilation and Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="app_development.html">Application Development</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running LLMs on the NPU</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="llm/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm/server_interface.html">Server Interface (REST API)</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm/high_level_python.html">High-Level Python SDK</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">OnnxRuntime GenAI (OGA) Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="oga_model_prepare.html">Preparing OGA Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Models on the GPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gpu/ryzenai_gpu.html">DirectML Flow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Additional Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="xrt_smi.html">NPU Management Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="ai_analyzer.html">AI Analyzer</a></li>
<li class="toctree-l1"><a class="reference internal" href="sd_demo.html">Stable Diffusion Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="ryzen_ai_libraries.html">Ryzen AI CVML library</a></li>
<li class="toctree-l1"><a class="reference external" href="https://huggingface.co/models?other=RyzenAI">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses.html">Licensing Information</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">OnnxRuntime GenAI (OGA) Flow</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>OnnxRuntime GenAI (OGA) Flow</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-configurations">Supported Configurations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements">Requirements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-optimized-models">Pre-optimized Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compatible-oga-apis">Compatible OGA APIs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llms-test-programs">LLMs Test Programs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-program">C++ Program</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-script">Python Script</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-c-applications">Building C++ Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-config-files">LLM Config Files</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-fine-tuned-models">Using Fine-Tuned Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-llm-via-pip-install">Running LLM via pip install</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="onnxruntime-genai-oga-flow">
<h1>OnnxRuntime GenAI (OGA) Flow<a class="headerlink" href="#onnxruntime-genai-oga-flow" title="Link to this heading">#</a></h1>
<p>Ryzen AI Software supports deploying LLMs on Ryzen AI PCs using the native ONNX Runtime Generate (OGA) C++ or Python API. The OGA API is the lowest-level API available for building LLM applications on a Ryzen AI PC. It supports the following execution modes:</p>
<ul class="simple">
<li><p>Hybrid execution mode: This mode uses both the NPU and iGPU to achieve the best TTFT and TPS during the prefill and decode phases.</p></li>
<li><p>NPU-only execution mode: This mode uses the NPU exclusively for both the prefill and decode phases.</p></li>
</ul>
<div class="pst-scrollable-table-container"><span id="software-stack-table"></span><table class="center-table table" id="id1">
<caption><span class="caption-text">Ryzen AI Software Stack</span><a class="headerlink" href="#id1" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Your Python Application</p></th>
<th class="head"><p>Your LLM Stack</p></th>
<th class="head"><p>Your Native Application</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="#high-level-python-sdk">Lemonade Python API*</a></p></td>
<td><p><a class="reference external" href="#server-interface-rest-api">Lemonade Server Interface*</a></p></td>
<td rowspan="2"><p> <a class="reference external" href="../hybrid_oga.html">OGA C++ Headers</a></p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p> <a class="reference external" href="https://onnxruntime.ai/docs/genai/api/python.html">OGA Python API*</a></p></td>
</tr>
<tr class="row-even"><td colspan="3"><p> <a class="reference external" href="https://github.com/microsoft/onnxruntime-genai">Custom AMD OnnxRuntime GenAI (OGA)</a></p></td>
</tr>
<tr class="row-odd"><td colspan="3"><p> <a class="reference external" href="https://www.amd.com/en/products/processors/consumer/ryzen-ai.html">AMD Ryzen AI Driver and Hardware</a></p></td>
</tr>
</tbody>
</table>
</div>
<p>* indicates open-source software (OSS).</p>
<section id="supported-configurations">
<h2>Supported Configurations<a class="headerlink" href="#supported-configurations" title="Link to this heading">#</a></h2>
<p>The Ryzen AI OGA flow supports Strix and Krackan Point processors. Phoenix (PHX) andÂ Hawk (HPT) processors are not supported.</p>
</section>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Install NPU Drivers and Ryzen AI MSI installer. See <a class="reference internal" href="inst.html"><span class="doc">Installation Instructions</span></a> for more details.</p></li>
<li><p>Install GPU device driver: Ensure GPU device driver <a class="reference external" href="https://www.amd.com/en/support">https://www.amd.com/en/support</a> is installed</p></li>
<li><p>Install Git for Windows (needed to download models from HF): <a class="reference external" href="https://git-scm.com/downloads">https://git-scm.com/downloads</a></p></li>
</ul>
</section>
<section id="pre-optimized-models">
<h2>Pre-optimized Models<a class="headerlink" href="#pre-optimized-models" title="Link to this heading">#</a></h2>
<p>AMD provides a set of pre-optimized LLMs ready to be deployed with Ryzen AI Software and the supporting runtime for hybrid and/or NPU only execution.</p>
<ul class="simple">
<li><p>Phi-3-mini-4k-instruct</p></li>
<li><p>Phi-3.5-mini-instruct</p></li>
<li><p>Mistral-7B-Instruct-v0.3</p></li>
<li><p>Qwen1.5-7B-Chat</p></li>
<li><p>chatglm3-6b</p></li>
<li><p>Llama-2-7b-hf</p></li>
<li><p>Llama-2-7b-chat-hf</p></li>
<li><p>Llama-3-8B</p></li>
<li><p>Llama-3.1-8B</p></li>
<li><p>Llama-3.2-1B-Instruct</p></li>
<li><p>Llama-3.2-3B-Instruct</p></li>
<li><p>Mistral-7B-Instruct-v0.1</p></li>
<li><p>Mistral-7B-Instruct-v0.2</p></li>
<li><p>Mistral-7B-v0.3</p></li>
<li><p>Llama-3.1-8B-Instruct</p></li>
<li><p>CodeLlama-7b-instruct-g128</p></li>
<li><p>DeepSeek-R1-Distill-Llama-8B</p></li>
<li><p>DeepSeek-R1-Distill-Qwen-1.5B</p></li>
<li><p>DeepSeek-R1-Distill-Qwen-7B</p></li>
<li><p>AMD-OLMo-1B-SFT-DPO</p></li>
<li><p>Qwen2-7B</p></li>
<li><p>Qwen2-1.5B</p></li>
<li><p>gemma-2-2b</p></li>
<li><p>Qwen2.5-1.5B-Instruct</p></li>
<li><p>Qwen2.5-3B-Instruct</p></li>
<li><p>Qwen2.5-7B-Instruct</p></li>
</ul>
<p>Hugging Face collection of hybrid models: <a class="reference external" href="https://huggingface.co/collections/amd/ryzenai-15-llm-hybrid-models-6859a64b421b5c27e1e53899">https://huggingface.co/collections/amd/ryzenai-15-llm-hybrid-models-6859a64b421b5c27e1e53899</a></p>
<p>Hugging Face collection of NPU models: <a class="reference external" href="https://huggingface.co/collections/amd/ryzenai-15-llm-npu-models-6859846d7c13f81298990db0">https://huggingface.co/collections/amd/ryzenai-15-llm-npu-models-6859846d7c13f81298990db0</a></p>
</section>
<section id="compatible-oga-apis">
<h2>Compatible OGA APIs<a class="headerlink" href="#compatible-oga-apis" title="Link to this heading">#</a></h2>
<p>Pre-optimized hybrid or NPU LLMs can be executed using the official OGA C++ and Python APIs. The current release is compatible with OGA version 0.7.0.
For detailed documentation and examples, refer to the official OGA repository:
ðŸ”— <a class="github reference external" href="https://github.com/microsoft/onnxruntime-genai/tree/rel-0.7.0">microsoft/onnxruntime-genai</a></p>
</section>
<section id="llms-test-programs">
<h2>LLMs Test Programs<a class="headerlink" href="#llms-test-programs" title="Link to this heading">#</a></h2>
<p>The Ryzen AI installation includes test programs (in C++ and Python) that can be used to run LLMs and understand how to integrate them in your application.</p>
<p>The steps for deploying the pre-optimized models using the sample programs are described in the following sections.</p>
<section id="c-program">
<h3>C++ Program<a class="headerlink" href="#c-program" title="Link to this heading">#</a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">model_benchmark.exe</span></code> executable to test LLMs and identify DLL dependencies for C++ applications.</p>
<ol class="arabic simple">
<li><p>(Optional) Enable Performance Mode</p></li>
</ol>
<p>To run LLMs in best performance mode, follow these steps:</p>
<ul>
<li><p>Go to <code class="docutils literal notranslate"><span class="pre">Windows</span></code> â†’ <code class="docutils literal notranslate"><span class="pre">Settings</span></code> â†’ <code class="docutils literal notranslate"><span class="pre">System</span></code> â†’ <code class="docutils literal notranslate"><span class="pre">Power</span></code>, and set the power mode to <strong>Best Performance</strong>.</p></li>
<li><p>Open a terminal and run:</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">cd</span> C:\Windows\System32\AMD
xrt-smi configure --pmode performance
</pre></div>
</div>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Activate the Ryzen AI 1.5.0 Conda Environment</p></li>
</ol>
<p>Run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>activate<span class="w"> </span>ryzen-ai-1.5.0
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Set Up a Working Directory and Copy Required Files</p></li>
</ol>
<p>Create a folder and copy the required files into it:</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">mkdir</span> llm_run
<span class="k">cd</span> llm_run

<span class="p">:</span><span class="c1">: Copy the sample C++ executable</span>
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\LLM\example\model_benchmark.exe&quot;</span> .

<span class="p">:</span><span class="c1">: Copy the sample prompt file</span>
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\LLM\example\amd_genai_prompt.txt&quot;</span> .

<span class="p">:</span><span class="c1">: Copy common DLLs</span>
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\onnxruntime-genai.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\onnxruntime.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\ryzen_mm.dll&quot;</span> .

<span class="p">:</span><span class="c1">: Copy DLLs for Hybrid models (skip if using an NPU-only model)</span>
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\onnx_custom_ops.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\libutf8_validity.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\abseil_dll.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\DirectML.dll&quot;</span> .

<span class="p">:</span><span class="c1">: Copy DLLs for NPU-only models (skip if using a Hybrid model)</span>
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\onnxruntime_providers_shared.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\onnxruntime_providers_vitisai.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\onnxruntime_vitis_ai_custom_ops.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\dyn_dispatch_core.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\xaiengine.dll&quot;</span> .
xcopy /Y <span class="s2">&quot;</span><span class="nv">%RYZEN_AI_INSTALLATION_PATH%</span><span class="s2">\deployment\onnxruntime_vitisai_ep.dll&quot;</span> .
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Download a Pre-Optimized Model from Hugging Face</p></li>
</ol>
<p>Use Git LFS to download the model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>::<span class="w"> </span>Install<span class="w"> </span>Git<span class="w"> </span>LFS<span class="w"> </span><span class="k">if</span><span class="w"> </span>you<span class="w"> </span>haven<span class="err">&#39;</span>t<span class="w"> </span>already:<span class="w"> </span>https://git-lfs.com
git<span class="w"> </span>lfs<span class="w"> </span>install

::<span class="w"> </span>Clone<span class="w"> </span>the<span class="w"> </span>model<span class="w"> </span>repository
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/amd/Llama-2-7b-chat-hf-awq-g128-int4-asym-fp16-onnx-hybrid
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">model_benchmark.exe</span></code></p></li>
</ol>
<p>Run the benchmark using the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>.<span class="se">\m</span>odel_benchmark.exe<span class="w"> </span>-i<span class="w"> </span>&lt;path_to_model_dir&gt;<span class="w"> </span>-f<span class="w"> </span>&lt;prompt_file&gt;<span class="w"> </span>-l<span class="w"> </span>&lt;list_of_prompt_lengths&gt;

::<span class="w"> </span>Example:
.<span class="se">\m</span>odel_benchmark.exe<span class="w"> </span>-i<span class="w"> </span>Llama-2-7b-chat-hf-awq-g128-int4-asym-fp16-onnx-hybrid<span class="w"> </span>-f<span class="w"> </span>amd_genai_prompt.txt<span class="w"> </span>-l<span class="w"> </span><span class="s2">&quot;1024&quot;</span>
</pre></div>
</div>
</section>
<section id="python-script">
<h3>Python Script<a class="headerlink" href="#python-script" title="Link to this heading">#</a></h3>
<p>Run sample python script</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python &quot;%RYZEN_AI_INSTALLATION_PATH%\LLM\example\run_model.py&quot; -m &lt;model_folder&gt; -l &lt;max_length&gt;

:: Example command
python &quot;%RYZEN_AI_INSTALLATION_PATH%\LLM\example\run_model.py&quot; -m &quot;Llama-2-7b-chat-hf-awq-g128-int4-asym-fp16-onnx-hybrid&quot; -l 256
</pre></div>
</div>
</section>
</section>
<section id="building-c-applications">
<h2>Building C++ Applications<a class="headerlink" href="#building-c-applications" title="Link to this heading">#</a></h2>
<p>A complete example including C++ source and build instructions is available in the RyzenAI-SW repository: <a class="github reference external" href="https://github.com/amd/RyzenAI-SW/tree/main/example/llm/oga_api">amd/RyzenAI-SW</a></p>
</section>
<section id="llm-config-files">
<h2>LLM Config Files<a class="headerlink" href="#llm-config-files" title="Link to this heading">#</a></h2>
<p>Each OGA model folder contains a <code class="docutils literal notranslate"><span class="pre">genai_config.json</span></code> file. This file contains various configuration settings for the model. The <code class="docutils literal notranslate"><span class="pre">session_option</span></code> section is where information about specific runtime dependencies is specified. Within this section, the <code class="docutils literal notranslate"><span class="pre">custom_ops_library</span></code> option sets the path to the <code class="docutils literal notranslate"><span class="pre">onnx_custom_ops.dll</span></code> file for Hybrid models and <code class="docutils literal notranslate"><span class="pre">onnxruntime_vitis_ai_custom_ops.dll</span></code> file for NPU models.</p>
<p>The following sample shows the defaults for the AMD pre-optimized Hybrid OGA LLMs:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;session_options&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;log_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;onnxruntime-genai&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;custom_ops_library&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;onnx_custom_ops.dll&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="err">...</span>
</pre></div>
</div>
<p>The paths is relative to the folder where the program is run from. The model throws an error if the <code class="docutils literal notranslate"><span class="pre">onnx_custom_ops.dll</span></code> file cannot be found at the specified location. Replacing the relative path with an absolute path to this file allows running the program from any location.</p>
</section>
<section id="using-fine-tuned-models">
<h2>Using Fine-Tuned Models<a class="headerlink" href="#using-fine-tuned-models" title="Link to this heading">#</a></h2>
<p>It is also possible to run fine-tuned versions of the pre-optimized OGA models.</p>
<p>To do this, the fine-tuned models must first be prepared for execution with the OGA Hybrid flow. For instructions on how to do this, refer to the page about <a class="reference internal" href="oga_model_prepare.html"><span class="doc">Preparing OGA Models</span></a>.</p>
<p>After a fine-tuned model has been prepared for Hybrid execution, it can be deployed by following the steps described previously in this page.</p>
</section>
<section id="running-llm-via-pip-install">
<h2>Running LLM via pip install<a class="headerlink" href="#running-llm-via-pip-install" title="Link to this heading">#</a></h2>
<p>In addition to the full RyzenAI software stack, we also provide standalone wheel files for the users who prefer using their own environment. To prepare an environment for running the Hybrid and NPU-only LLM independently, perform the following steps:</p>
<ol class="arabic simple">
<li><p>Create a new python environment and activate it.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>&lt;env_name&gt;<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="w"> </span>-y
conda<span class="w"> </span>activate<span class="w"> </span>&lt;env_name&gt;
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Install onnxruntime-genai wheel file.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>onnxruntime-genai-directml-ryzenai<span class="o">==</span><span class="m">0</span>.7.0.2<span class="w"> </span>--extra-index-url<span class="o">=</span>https://pypi.amd.com/simple
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Navigate to your working directory and download the desired Hybrid/NPU model</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>working_directory
git<span class="w"> </span>clone<span class="w"> </span>&lt;link_to_model&gt;
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Copy the required DLLs from the current environment folder.</p></li>
</ol>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="p">:</span><span class="c1">: Copy DLLs for Hybrid models (skip if using an NPU-only model)</span>
xcopy <span class="s2">&quot;</span><span class="nv">%CONDA_PREFIX%</span><span class="s2">\Lib\site-packages\onnxruntime_genai\onnx_custom_ops.dll&quot;</span> .
xcopy <span class="s2">&quot;</span><span class="nv">%CONDA_PREFIX%</span><span class="s2">\Lib\site-packages\onnxruntime_genai\libutf8_validity.dll&quot;</span> .
xcopy <span class="s2">&quot;</span><span class="nv">%CONDA_PREFIX%</span><span class="s2">\Lib\site-packages\onnxruntime_genai\abseil_dll.dll&quot;</span> .

<span class="p">:</span><span class="c1">: Copy DLLs for NPU-only models (skip if using a Hybrid model)</span>
xcopy <span class="s2">&quot;</span><span class="nv">%CONDA_PREFIX%</span><span class="s2">\Lib\site-packages\onnxruntime\capi\onnxruntime_vitis_ai_custom_ops.dll&quot;</span> .
xcopy <span class="s2">&quot;</span><span class="nv">%CONDA_PREFIX%</span><span class="s2">\Lib\site-packages\onnxruntime\capi\dyn_dispatch_core.dll&quot;</span> .
xcopy <span class="s2">&quot;</span><span class="nv">%CONDA_PREFIX%</span><span class="s2">\Lib\site-packages\onnxruntime\capi\xaiengine.dll&quot;</span> .
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Run the Hybrid or NPU model.</p></li>
</ol>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="llm/high_level_python.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">High-Level Python SDK</p>
      </div>
    </a>
    <a class="right-next"
       href="oga_model_prepare.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Preparing OGA Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-configurations">Supported Configurations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements">Requirements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-optimized-models">Pre-optimized Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compatible-oga-apis">Compatible OGA APIs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llms-test-programs">LLMs Test Programs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-program">C++ Program</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-script">Python Script</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-c-applications">Building C++ Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-config-files">LLM Config Files</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-fine-tuned-models">Using Fine-Tuned Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-llm-via-pip-install">Running LLM via pip install</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on June 29, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://ryzenai.docs.amd.com/en/latest/licenses.html">Ryzen AI Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">Â© 2025 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>