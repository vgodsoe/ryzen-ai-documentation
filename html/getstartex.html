
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Getting Started Tutorial &#8212; Ryzen AI Software 1.5 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=643846e8" />
    <link rel="stylesheet" type="text/css" href="_static/llm-table.css?v=d34acfde" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_header.css?v=9557e3d1" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_footer.css?v=7095035a" />
    <link rel="stylesheet" type="text/css" href="_static/fonts.css?v=fcff5274" />
    <link rel="stylesheet" type="text/css" href="_static/_static/llm-table.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9f1c6d22"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script defer="defer" src="_static/search.js?v=90a4452c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'getstartex';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="canonical" href="ryzenai.docs.amd.com/getstartex.html" />
    <link rel="icon" href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="google-site-verification" content="vo35SZt_GASsTHAEmdww7AYKPCvZyzLvOXBl8guBME4" />

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="/">Ryzen AI</a>
                
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/vgodsoe/ryzen-ai-documentation" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://community.amd.com/t5/ai/ct-p/amd_ai" id="navcommunity" role="button" aria-expanded="false" target="_blank" >
                                Community
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://www.amd.com/en/products/ryzen-ai" id="navproducts" role="button" aria-expanded="false" target="_blank" >
                                Products
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Ryzen AI Software 1.5 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="relnotes.html">Release Notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="inst.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples, Demos, Tutorials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Models on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="model_quantization.html">Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="modelrun.html">Model Compilation and Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="app_development.html">Application Development</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running LLMs on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="llm/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm/server_interface.html">Server Interface (REST API)</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm/high_level_python.html">High-Level Python SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="hybrid_oga.html">OnnxRuntime GenAI (OGA) Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="oga_model_prepare.html">Preparing OGA Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Models on the GPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gpu/ryzenai_gpu.html">DirectML Flow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Additional Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="xrt_smi.html">NPU Management Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="ai_analyzer.html">AI Analyzer</a></li>
<li class="toctree-l1"><a class="reference internal" href="sd_demo.html">Stable Diffusion Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="ryzen_ai_libraries.html">Ryzen AI CVML library</a></li>
<li class="toctree-l1"><a class="reference external" href="https://huggingface.co/models?other=RyzenAI">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses.html">Licensing Information</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Getting Started Tutorial</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Getting Started Tutorial</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-install-packages">Step 1: Install Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-dataset-and-onnx-model">Step 2: Prepare dataset and ONNX model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-quantize-the-model">Step 3: Quantize the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-deploy-the-model">Step 4: Deploy the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-python">Deployment - Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-cpu">Deploy the Model on the CPU</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-ryzen-ai-npu">Deploy the Model on the Ryzen AI NPU</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-c">Deployment - C++</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#install-opencv">Install OpenCV</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-run-custom-resnet-c-sample">Build and Run Custom Resnet C++ sample</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deploy the Model on the CPU</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-npu">Deploy the Model on the NPU</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="getting-started-tutorial">
<h1>Getting Started Tutorial<a class="headerlink" href="#getting-started-tutorial" title="Link to this heading">#</a></h1>
<p>This tutorial uses a fine-tuned version of the ResNet model (using the CIFAR-10 dataset) to demonstrate the process of preparing, quantizing, and deploying a model using Ryzen AI Software. The tutorial features deployment using both Python and C++ ONNX runtime code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this documentation, “NPU” is used in descriptions, while “IPU” is retained in some of the tool’s language, code, screenshots, and commands. This intentional
distinction aligns with existing tool references and does not affect functionality. Avoid making replacements in the code.</p>
</div>
<ul class="simple">
<li><p>The source code files can be downloaded from <a class="reference external" href="https://github.com/amd/RyzenAI-SW/tree/main/tutorial/getting_started_resnet">this link</a>. Alternatively, you can clone the RyzenAI-SW repo and change the directory into “tutorial”.</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/amd/RyzenAI-SW.git
cd tutorial/getting_started_resnet
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The following are the steps and the required files to run the example:</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 28.6%" />
<col style="width: 35.7%" />
<col style="width: 35.7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Steps</p></th>
<th class="head"><p>Files Used</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Installation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code></p></td>
<td><p>Install the necessary package for this example.</p></td>
</tr>
<tr class="row-odd"><td><p>Preparation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">prepare_model_data.py</span></code>,
<code class="docutils literal notranslate"><span class="pre">resnet_utils.py</span></code></p></td>
<td><p>The script <code class="docutils literal notranslate"><span class="pre">prepare_model_data.py</span></code> prepares the model and the data for the rest of the tutorial.</p>
<ol class="arabic simple">
<li><p>To prepare the model the script converts pre-trained PyTorch model to ONNX format.</p></li>
<li><p>To prepare the necessary data the script downloads and extracts CIFAR-10 dataset.</p></li>
</ol>
</td>
</tr>
<tr class="row-even"><td><p>Pretrained model</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">models/resnet_trained_for_cifar10.pt</span></code></p></td>
<td><p>The ResNet model trained using CIFAR-10 is provided in .pt format.</p></td>
</tr>
<tr class="row-odd"><td><p>Quantization</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">resnet_quantize.py</span></code></p></td>
<td><p>Convert the model to the NPU-deployable model by performing Post-Training Quantization flow using AMD Quark Quantization.</p></td>
</tr>
<tr class="row-even"><td><p>Deployment - Python</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">predict.py</span></code></p></td>
<td><p>Run the Quantized model using the ONNX Runtime code. It demonstrates running the model on both CPU and NPU.</p></td>
</tr>
<tr class="row-odd"><td><p>Deployment - C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cpp/resnet_cifar/.</span></code></p></td>
<td><p>This folder contains the source code <code class="docutils literal notranslate"><span class="pre">resnet_cifar.cpp</span></code> that demonstrates running inference using C++ APIs. Additionally, the infrastructure (required libraries, CMake files and header files) required by the example are provided.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<section id="step-1-install-packages">
<h2>Step 1: Install Packages<a class="headerlink" href="#step-1-install-packages" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Ensure that the Ryzen AI Software  is correctly installed. For more details, see the <a class="reference internal" href="inst.html"><span class="doc">installation instructions</span></a>.</p></li>
<li><p>Use the conda environment created during the installation for the rest of the steps. This example requires a couple of additional packages. Run the following command to install them:</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python -m pip install -r requirements.txt
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="step-2-prepare-dataset-and-onnx-model">
<h2>Step 2: Prepare dataset and ONNX model<a class="headerlink" href="#step-2-prepare-dataset-and-onnx-model" title="Link to this heading">#</a></h2>
<p>This example utilizes a custom ResNet model finetuned using the CIFAR-10 dataset</p>
<p>The <code class="docutils literal notranslate"><span class="pre">prepare_model_data.py</span></code> script downloads the CIFAR-10 dataset in pickle format (for python) and binary format (for C++). This dataset is used in the subsequent steps for quantization and inference. The script also exports the provided PyTorch model into ONNX format. The following snippet from the script shows how the ONNX model is exported:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dummy_inputs = torch.randn(1, 3, 32, 32)
input_names = [&#39;input&#39;]
output_names = [&#39;output&#39;]
dynamic_axes = {&#39;input&#39;: {0: &#39;batch_size&#39;}, &#39;output&#39;: {0: &#39;batch_size&#39;}}
tmp_model_path = str(models_dir / &quot;resnet_trained_for_cifar10.onnx&quot;)
torch.onnx.export(
        model,
        dummy_inputs,
        tmp_model_path,
        export_params=True,
        opset_version=13,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
    )
</pre></div>
</div>
<p>Note the following settings for the onnx conversion:</p>
<ul class="simple">
<li><p>Ryzen AI supports a batch size=1, so dummy input is fixed to a batch_size =1 during model conversion</p></li>
<li><p>Recommended <code class="docutils literal notranslate"><span class="pre">opset_version</span></code> setting 13 is used.</p></li>
</ul>
<p>Run the following command to prepare the dataset and export the ONNX model:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python prepare_model_data.py
</pre></div>
</div>
<ul class="simple">
<li><p>The downloaded CIFAR-10 dataset is saved in the current directory at the following location: <code class="docutils literal notranslate"><span class="pre">data/*</span></code>.</p></li>
<li><p>The ONNX model is generated at models/resnet_trained_for_cifar10.onnx</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="step-3-quantize-the-model">
<h2>Step 3: Quantize the Model<a class="headerlink" href="#step-3-quantize-the-model" title="Link to this heading">#</a></h2>
<p>Quantizing AI models from floating-point to 8-bit integers reduces computational power and the memory footprint required for inference. This example utilizes Quark for ONNX quantizer workflow. Quark takes the pre-trained float32 model from the previous step (<code class="docutils literal notranslate"><span class="pre">resnet_trained_for_cifar10.onnx</span></code>) and provides a quantized model.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python resnet_quantize.py
</pre></div>
</div>
<p>This generates a quantized model using QDQ quant format and generate Quantized model with default configuration. After the completion of the run, the quantized ONNX model <code class="docutils literal notranslate"><span class="pre">resnet_quantized.onnx</span></code> is saved to models/resnet_quantized.onnx</p>
<p>The <code class="file docutils literal notranslate"><span class="pre">resnet_quantize.py</span></code> file has <code class="docutils literal notranslate"><span class="pre">ModelQuantizer::quantize_model</span></code> function that applies quantization to the model.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from quark.onnx.quantization.config import (Config, get_default_config)
from quark.onnx import ModelQuantizer

# Get quantization configuration
quant_config = get_default_config(&quot;XINT8&quot;)
config = Config(global_quant_config=quant_config)

# Create an ONNX quantizer
quantizer = ModelQuantizer(config)

# Quantize the ONNX model
quantizer.quantize_model(input_model_path, output_model_path, dr)
</pre></div>
</div>
<p>The parameters of this function are:</p>
<ul class="simple">
<li><p><strong>input_model_path</strong>: (String) The file path of the model to be quantized.</p></li>
<li><p><strong>output_model_path</strong>: (String) The file path where the quantized model is saved.</p></li>
<li><p><strong>dr</strong>: (Object or None) Calibration data reader that enumerates the calibration data and producing inputs for the original model. In this example, CIFAR10 dataset is used for calibration during the quantization process.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="step-4-deploy-the-model">
<h2>Step 4: Deploy the Model<a class="headerlink" href="#step-4-deploy-the-model" title="Link to this heading">#</a></h2>
<p>It demonstrates deploying the quantized model using both Python and C++ APIs.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#dep-python"><span class="std std-ref">Deployment - Python</span></a></p></li>
<li><p><a class="reference internal" href="#dep-cpp"><span class="std std-ref">Deployment - C++</span></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During the Python and C++ deployment, the compiled model artifacts are saved in the cache folder named <code class="docutils literal notranslate"><span class="pre">&lt;run</span> <span class="pre">directory&gt;/modelcachekey</span></code>. Ryzen AI does not support the complied model artifacts across the versions, so if the model artifacts exist from the previous software version, ensure to delete the <code class="docutils literal notranslate"><span class="pre">modelcachekey</span></code> folder before executing the deployment steps.</p>
</div>
<section id="deployment-python">
<span id="dep-python"></span><h3>Deployment - Python<a class="headerlink" href="#deployment-python" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">predict.py</span></code> script is used to deploy the model. It extracts the first ten images from the CIFAR-10 test dataset and converts them to the .png format. The script then reads all those ten images and classifies them by running the quantized custom ResNet model on CPU or NPU.</p>
<section id="deploy-the-model-on-the-cpu">
<h4>Deploy the Model on the CPU<a class="headerlink" href="#deploy-the-model-on-the-cpu" title="Link to this heading">#</a></h4>
<p>By default, <code class="docutils literal notranslate"><span class="pre">predict.py</span></code> runs the model on CPU.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python predict.py
</pre></div>
</div>
<p>Typical output</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Image 0: Actual Label cat, Predicted Label cat
Image 1: Actual Label ship, Predicted Label ship
Image 2: Actual Label ship, Predicted Label airplane
Image 3: Actual Label airplane, Predicted Label airplane
Image 4: Actual Label frog, Predicted Label frog
Image 5: Actual Label frog, Predicted Label frog
Image 6: Actual Label automobile, Predicted Label automobile
Image 7: Actual Label frog, Predicted Label frog
Image 8: Actual Label cat, Predicted Label cat
Image 9: Actual Label automobile, Predicted Label automobile
</pre></div>
</div>
</section>
<section id="deploy-the-model-on-the-ryzen-ai-npu">
<h4>Deploy the Model on the Ryzen AI NPU<a class="headerlink" href="#deploy-the-model-on-the-ryzen-ai-npu" title="Link to this heading">#</a></h4>
<p>To successfully run the model on the NPU, follow these setup steps:</p>
<ul class="simple">
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> points to <code class="docutils literal notranslate"><span class="pre">path\to\ryzen-ai-sw-&lt;version&gt;\</span></code>. If you installed Ryzen AI software using the MSI installer, this variable should already be set. Ensure that the Ryzen AI software package has not been moved post installation, in which case <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> has to be set again.</p></li>
<li><p>The binary for inference session need to be explicitly passed through the <cite>xclbin</cite> option in provider_options</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>parser = argparse.ArgumentParser()
parser.add_argument(&#39;--ep&#39;, type=str, default =&#39;cpu&#39;,choices = [&#39;cpu&#39;,&#39;npu&#39;], help=&#39;EP backend selection&#39;)
opt = parser.parse_args()

providers = [&#39;CPUExecutionProvider&#39;]
provider_options = [{}]

if opt.ep == &#39;npu&#39;:
   providers = [&#39;VitisAIExecutionProvider&#39;]
   cache_dir = Path(__file__).parent.resolve()
   provider_options = [{
              &#39;cacheDir&#39;: str(cache_dir),
              &#39;cacheKey&#39;: &#39;modelcachekey&#39;,
              &#39;xclbin&#39;: &#39;path/to/xclbin&#39;
              }]

session = ort.InferenceSession(model.SerializeToString(), providers=providers,
                               provider_options=provider_options)
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">predict.py</span></code> with the <code class="docutils literal notranslate"><span class="pre">--ep</span> <span class="pre">npu</span></code> switch to run the custom ResNet model on the Ryzen AI NPU:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python predict.py --ep npu
</pre></div>
</div>
<p>Typical output</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[Vitis AI EP] No. of Operators :   CPU     2    IPU   398  99.50%
[Vitis AI EP] No. of Subgraphs :   CPU     1    IPU     1 Actually running on IPU     1
...
Image 0: Actual Label cat, Predicted Label cat
Image 1: Actual Label ship, Predicted Label ship
Image 2: Actual Label ship, Predicted Label ship
Image 3: Actual Label airplane, Predicted Label airplane
Image 4: Actual Label frog, Predicted Label frog
Image 5: Actual Label frog, Predicted Label frog
Image 6: Actual Label automobile, Predicted Label truck
Image 7: Actual Label frog, Predicted Label frog
Image 8: Actual Label cat, Predicted Label cat
Image 9: Actual Label automobile, Predicted Label automobile
</pre></div>
</div>
</section>
</section>
<section id="deployment-c">
<span id="dep-cpp"></span><h3>Deployment - C++<a class="headerlink" href="#deployment-c" title="Link to this heading">#</a></h3>
<section id="prerequisites">
<h4>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>Visual Studio 2022 Community edition, ensure <strong>Desktop Development with C++</strong> is installed</p></li>
<li><p>cmake (version &gt;= 3.26)</p></li>
<li><p>opencv (version=4.6.0) required for the custom resnet example</p></li>
</ol>
</section>
<section id="install-opencv">
<h4>Install OpenCV<a class="headerlink" href="#install-opencv" title="Link to this heading">#</a></h4>
<p>It is recommended to build OpenCV from the source code and use static build. The default installation location is “install” , the following instruction installs OpenCV in the location “C:\opencv” as an example. You may first change the directory to where you want to clone the OpenCV repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/opencv/opencv.git<span class="w"> </span>-b<span class="w"> </span><span class="m">4</span>.6.0
<span class="nb">cd</span><span class="w"> </span>opencv
cmake<span class="w"> </span>-DCMAKE_EXPORT_COMPILE_COMMANDS<span class="o">=</span>ON<span class="w"> </span>-DBUILD_SHARED_LIBS<span class="o">=</span>OFF<span class="w"> </span>-DCMAKE_POSITION_INDEPENDENT_CODE<span class="o">=</span>ON<span class="w"> </span>-DCMAKE_CONFIGURATION_TYPES<span class="o">=</span>Release<span class="w"> </span>-A<span class="w"> </span>x64<span class="w"> </span>-T<span class="w"> </span><span class="nv">host</span><span class="o">=</span>x64<span class="w"> </span>-G<span class="w"> </span><span class="s2">&quot;Visual Studio 17 2022&quot;</span><span class="w"> </span><span class="s2">&quot;-DCMAKE_INSTALL_PREFIX=C:\opencv&quot;</span><span class="w"> </span><span class="s2">&quot;-DCMAKE_PREFIX_PATH=C:\opencv&quot;</span><span class="w"> </span>-DCMAKE_BUILD_TYPE<span class="o">=</span>Release<span class="w"> </span>-DBUILD_opencv_python2<span class="o">=</span>OFF<span class="w"> </span>-DBUILD_opencv_python3<span class="o">=</span>OFF<span class="w"> </span>-DBUILD_WITH_STATIC_CRT<span class="o">=</span>OFF<span class="w"> </span>-B<span class="w"> </span>build
cmake<span class="w"> </span>--build<span class="w"> </span>build<span class="w"> </span>--config<span class="w"> </span>Release
cmake<span class="w"> </span>--install<span class="w"> </span>build<span class="w"> </span>--config<span class="w"> </span>Release
</pre></div>
</div>
<p>The build files are written to <code class="docutils literal notranslate"><span class="pre">build\</span></code>.</p>
</section>
<section id="build-and-run-custom-resnet-c-sample">
<h4>Build and Run Custom Resnet C++ sample<a class="headerlink" href="#build-and-run-custom-resnet-c-sample" title="Link to this heading">#</a></h4>
<p>The C++ source files, CMake list files, and related artifacts are provided in the <code class="docutils literal notranslate"><span class="pre">cpp/resnet_cifar/*</span></code> folder. The source file <code class="docutils literal notranslate"><span class="pre">cpp/resnet_cifar/resnet_cifar.cpp</span></code> takes 10 images from the CIFAR-10 test set, converts them to .png format, preprocesses them, and performs model inference. The example has onnxruntime dependencies that are provided in <code class="docutils literal notranslate"><span class="pre">%RYZEN_AI_INSTALLATION_PATH%/onnxruntime/*</span></code>.</p>
<p>Run the following command to build the resnet example. Assign <code class="docutils literal notranslate"><span class="pre">-DOpenCV_DIR</span></code> to the OpenCV build directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>getting_started_resnet/cpp
cmake<span class="w"> </span>-DCMAKE_EXPORT_COMPILE_COMMANDS<span class="o">=</span>ON<span class="w"> </span>-DBUILD_SHARED_LIBS<span class="o">=</span>OFF<span class="w"> </span>-DCMAKE_POSITION_INDEPENDENT_CODE<span class="o">=</span>ON<span class="w"> </span>-DCMAKE_CONFIGURATION_TYPES<span class="o">=</span>Release<span class="w"> </span>-A<span class="w"> </span>x64<span class="w"> </span>-T<span class="w"> </span><span class="nv">host</span><span class="o">=</span>x64<span class="w"> </span>-DCMAKE_INSTALL_PREFIX<span class="o">=</span>.<span class="w"> </span>-DCMAKE_PREFIX_PATH<span class="o">=</span>.<span class="w"> </span>-B<span class="w"> </span>build<span class="w"> </span>-S<span class="w"> </span>resnet_cifar<span class="w"> </span>-DOpenCV_DIR<span class="o">=</span><span class="s2">&quot;C:/opencv/build&quot;</span><span class="w"> </span>-G<span class="w"> </span><span class="s2">&quot;Visual Studio 17 2022&quot;</span>
</pre></div>
</div>
<p>This should generate the build directory with the <code class="docutils literal notranslate"><span class="pre">resnet_cifar.sln</span></code> solution file along with other project files. Open the solution file using Visual Studio 2022 and build to compile. You can also use “Developer Command Prompt for VS 2022” to open the solution file in Visual Studio.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>devenv<span class="w"> </span>build/resnet_cifar.sln
</pre></div>
</div>
<p>Now to deploy the model, go back to the parent directory (getting_started_resnet) of this example. After compilation, the executable should be generated in <code class="docutils literal notranslate"><span class="pre">cpp/build/Release/resnet_cifar.exe</span></code>. Copy this application over to the parent directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>..
xcopy<span class="w"> </span>cpp<span class="se">\b</span>uild<span class="se">\R</span>elease<span class="se">\r</span>esnet_cifar.exe<span class="w"> </span>.
</pre></div>
</div>
<p>Additionally, copy the onnxruntime DLLs from the Vitis AI Execution Provider package to the current directory. The following commands copy the required files in the current directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xcopy<span class="w"> </span>%RYZEN_AI_INSTALLATION_PATH%<span class="se">\o</span>nnxruntime<span class="se">\b</span>in<span class="se">\*</span><span class="w"> </span>/E<span class="w"> </span>/I
</pre></div>
</div>
<p>The C++ application that was generated takes three arguments:</p>
<ol class="arabic simple">
<li><p>Path to the quantized ONNX model generated in Step three</p></li>
<li><p>The execution provider of choice (cpu or NPU)</p></li>
<li><p>vaip_config.json (pass None if running on CPU)</p></li>
</ol>
<section id="id1">
<h5>Deploy the Model on the CPU<a class="headerlink" href="#id1" title="Link to this heading">#</a></h5>
<p>To run the model on the CPU, use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>resnet_cifar.exe<span class="w"> </span>models<span class="se">\r</span>esnet_quantized.onnx<span class="w"> </span>cpu
</pre></div>
</div>
<p>Typical output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>model<span class="w"> </span>name:models<span class="se">\r</span>esnet_quantized.onnx
ep:cpu
Input<span class="w"> </span>Node<span class="w"> </span>Name/Shape<span class="w"> </span><span class="o">(</span><span class="m">1</span><span class="o">)</span>:
<span class="w">        </span>input<span class="w"> </span>:<span class="w"> </span>-1x3x32x32
Output<span class="w"> </span>Node<span class="w"> </span>Name/Shape<span class="w"> </span><span class="o">(</span><span class="m">1</span><span class="o">)</span>:
<span class="w">        </span>output<span class="w"> </span>:<span class="w"> </span>-1x10
Final<span class="w"> </span>results:
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>airplane<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>airplane
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>truck<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>automobile
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>automobile<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>automobile
</pre></div>
</div>
</section>
<section id="deploy-the-model-on-the-npu">
<h5>Deploy the Model on the NPU<a class="headerlink" href="#deploy-the-model-on-the-npu" title="Link to this heading">#</a></h5>
<p>To successfully run the model on the NPU:</p>
<ul class="simple">
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> points to <code class="docutils literal notranslate"><span class="pre">path\to\ryzen-ai-sw-&lt;version&gt;\</span></code>. If you installed Ryzen AI software using the MSI installer, this variable should already be set. Ensure that the Ryzen AI software package has not been moved post installation, in which case <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> has to be set again.</p></li>
<li><p>The binary for inference session need to be explicitly passed through the <cite>xclbin</cite> option in provider_options</p></li>
</ul>
<p>The following code block from <code class="docutils literal notranslate"><span class="pre">reset_cifar.cpp</span></code> shows how ONNX Runtime is configured to deploy the model on the Ryzen AI NPU:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>auto<span class="w"> </span><span class="nv">session_options</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>Ort::SessionOptions<span class="o">()</span><span class="p">;</span>

auto<span class="w"> </span><span class="nv">cache_dir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>std::filesystem::current_path<span class="o">()</span>.string<span class="o">()</span><span class="p">;</span>

<span class="k">if</span><span class="o">(</span><span class="nv">ep</span><span class="o">==</span><span class="s2">&quot;npu&quot;</span><span class="o">)</span>
<span class="o">{</span>
auto<span class="w"> </span><span class="nv">options</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span>std::unordered_map&lt;std::string,<span class="w"> </span>std::string&gt;<span class="o">{</span><span class="w"> </span><span class="o">{</span><span class="s2">&quot;cacheDir&quot;</span>,<span class="w"> </span>cache_dir<span class="o">}</span>,<span class="w"> </span><span class="o">{</span><span class="s2">&quot;cacheKey&quot;</span>,<span class="w"> </span><span class="s2">&quot;modelcachekey&quot;</span><span class="o">}</span>,<span class="w"> </span><span class="o">{</span><span class="s2">&quot;xclbin&quot;</span>,<span class="w"> </span><span class="s2">&quot;path/to/xclbin&quot;</span><span class="o">}}</span><span class="p">;</span>
session_options.AppendExecutionProvider_VitisAI<span class="o">(</span>options<span class="o">)</span>
<span class="o">}</span>

auto<span class="w"> </span><span class="nv">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>Ort::Session<span class="o">(</span>env,<span class="w"> </span>model_name.data<span class="o">()</span>,<span class="w"> </span>session_options<span class="o">)</span><span class="p">;</span>
</pre></div>
</div>
<p>To run the model on the NPU, pass the npu flag and the vaip_config.json file as arguments to the C++ application. Use the following command to run the model on the NPU:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>resnet_cifar.exe<span class="w"> </span>models<span class="se">\r</span>esnet_quantized.onnx<span class="w"> </span>npu
</pre></div>
</div>
<p>Typical output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[Vitis AI EP] No. of Operators :   CPU     2    IPU   398  99.50%
[Vitis AI EP] No. of Subgraphs :   CPU     1    IPU     1 Actually running on IPU     1
...
Final results:
Predicted label is cat and actual label is cat
Predicted label is ship and actual label is ship
Predicted label is ship and actual label is ship
Predicted label is airplane and actual label is airplane
Predicted label is frog and actual label is frog
Predicted label is frog and actual label is frog
Predicted label is truck and actual label is automobile
Predicted label is frog and actual label is frog
Predicted label is cat and actual label is cat
Predicted label is automobile and actual label is automobile
</pre></div>
</div>
</section>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-install-packages">Step 1: Install Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-dataset-and-onnx-model">Step 2: Prepare dataset and ONNX model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-quantize-the-model">Step 3: Quantize the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-deploy-the-model">Step 4: Deploy the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-python">Deployment - Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-cpu">Deploy the Model on the CPU</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-ryzen-ai-npu">Deploy the Model on the Ryzen AI NPU</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-c">Deployment - C++</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#install-opencv">Install OpenCV</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-run-custom-resnet-c-sample">Build and Run Custom Resnet C++ sample</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deploy the Model on the CPU</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-npu">Deploy the Model on the NPU</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on June 29, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://ryzenai.docs.amd.com/en/latest/licenses.html">Ryzen AI Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2025 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>