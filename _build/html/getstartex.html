

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Getting Started Tutorial &#8212; Ryzen AI Software 1.3 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/llm-table.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_header.css" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_footer.css" />
    <link rel="stylesheet" type="text/css" href="_static/fonts.css" />
    <link rel="stylesheet" type="text/css" href="_static/_static/llm-table.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <script async="async" src="_static/code_word_breaks.js"></script>
    <script async="async" src="_static/renameVersionLinks.js"></script>
    <script async="async" src="_static/rdcMisc.js"></script>
    <script async="async" src="_static/theme_mode_captions.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'getstartex';</script>
    <link rel="canonical" href="ryzenai.docs.amd.com/getstartex.html" />
    <link rel="shortcut icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="google-site-verification" content="ZOn0RZC0Pwzlmf2SaE0bRttWk1YzOhuslbpxUDchQ90" />

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="/">Ryzen AI</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
                <!-- TODO: Search icon up here maybe? -->
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/vgodsoe/ryzen-ai-documentation" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://community.amd.com/t5/ai/ct-p/amd_ai" id="navcommunity" role="button" aria-expanded="false" target="_blank" >
                                Community
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://www.amd.com/en/products/ryzen-ai" id="navproducts" role="button" aria-expanded="false" target="_blank" >
                                Products
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    <p class="title logo__title">Ryzen AI Software 1.3 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="relnotes.html">Release Notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="inst.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_setup.html">Runtime Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples, Demos, Tutorials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running CNNs on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modelcompat.html">Model Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="modelport.html">Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="modelrun.html">Model Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="app_development.html">Application Development</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Models on the GPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gpu/ryzenai_gpu.html">DirectML Flow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running LLMs on the NPU</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="llm/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm/high_level_python.html">High-Level Python SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm/server_interface.html">Server Interface (REST API)</a></li>
<li class="toctree-l1"><a class="reference internal" href="hybrid_oga.html">OGA API for C++ and Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Additional Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="xrt_smi.html">NPU Management Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="ai_analyzer.html">AI Analyzer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Additional Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://huggingface.co/models?other=RyzenAI">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_installation.html">Manual Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="licenses.html">Licensing Information</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Getting...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">

<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Getting Started Tutorial</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-install-packages">Step 1: Install Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-dataset-and-onnx-model">Step 2: Prepare dataset and ONNX model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-quantize-the-model">Step 3: Quantize the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-deploy-the-model">Step 4: Deploy the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-python">Deployment - Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-cpu">Deploy the Model on the CPU</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-ryzen-ai-npu">Deploy the Model on the Ryzen AI NPU</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-c">Deployment - C++</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#install-opencv">Install OpenCV</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-run-custom-resnet-c-sample">Build and Run Custom Resnet C++ sample</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deploy the Model on the CPU</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-npu">Deploy the Model on the NPU</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="getting-started-tutorial">
<h1>Getting Started Tutorial<a class="headerlink" href="#getting-started-tutorial" title="Permalink to this heading">#</a></h1>
<p>This tutorial uses a fine-tuned version of the ResNet model (using the CIFAR-10 dataset) to demonstrate the process of preparing, quantizing, and deploying a model using Ryzen AI Software. The tutorial features deployment using both Python and C++ ONNX runtime code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this documentation, “NPU” is used in descriptions, while “IPU” is retained in some of the tool’s language, code, screenshots, and commands. This intentional
distinction aligns with existing tool references and does not affect functionality. Avoid making replacements in the code.</p>
</div>
<ul class="simple">
<li><p>The source code files can be downloaded from <a class="reference external" href="https://github.com/amd/RyzenAI-SW/tree/main/tutorial/getting_started_resnet">this link</a>. Alternatively, you can clone the RyzenAI-SW repo and change the directory into “tutorial”.</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/amd/RyzenAI-SW.git
cd tutorial/getting_started_resnet
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The following are the steps and the required files to run the example:</p>
<table class="table">
<colgroup>
<col style="width: 28.6%" />
<col style="width: 35.7%" />
<col style="width: 35.7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Steps</p></th>
<th class="head"><p>Files Used</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Installation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code></p></td>
<td><p>Install the necessary package for this example.</p></td>
</tr>
<tr class="row-odd"><td><p>Preparation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">prepare_model_data.py</span></code>,
<code class="docutils literal notranslate"><span class="pre">resnet_utils.py</span></code></p></td>
<td><p>The script <code class="docutils literal notranslate"><span class="pre">prepare_model_data.py</span></code> prepares the model and the data for the rest of the tutorial.</p>
<ol class="arabic simple">
<li><p>To prepare the model the script converts pre-trained PyTorch model to ONNX format.</p></li>
<li><p>To prepare the necessary data the script downloads and extract CIFAR-10 dataset.</p></li>
</ol>
</td>
</tr>
<tr class="row-even"><td><p>Pretrained model</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">models/resnet_trained_for_cifar10.pt</span></code></p></td>
<td><p>The ResNet model trained using CIFAR-10 is provided in .pt format.</p></td>
</tr>
<tr class="row-odd"><td><p>Quantization</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">resnet_quantize.py</span></code></p></td>
<td><p>Convert the model to the NPU-deployable model by performing Post-Training Quantization flow using AMD Quark Quantization.</p></td>
</tr>
<tr class="row-even"><td><p>Deployment - Python</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">predict.py</span></code></p></td>
<td><p>Run the Quantized model using the ONNX Runtime code. We demonstrate running the model on both CPU and NPU.</p></td>
</tr>
<tr class="row-odd"><td><p>Deployment - C++</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cpp/resnet_cifar/.</span></code></p></td>
<td><p>This folder contains the source code <code class="docutils literal notranslate"><span class="pre">resnet_cifar.cpp</span></code> that demonstrates running inference using C++ APIs. We additionally provide the infrastructure (required libraries, CMake files and headerfiles) required by the example.</p></td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<section id="step-1-install-packages">
<h2>Step 1: Install Packages<a class="headerlink" href="#step-1-install-packages" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Ensure that the Ryzen AI Software  is correctly installed. For more details, see the <a class="reference internal" href="inst.html"><span class="doc">installation instructions</span></a>.</p></li>
<li><p>Use the conda environment created during the installation for the rest of the steps. This example requires a couple of additional packages. Run the following command to install them:</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python -m pip install -r requirements.txt
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="step-2-prepare-dataset-and-onnx-model">
<h2>Step 2: Prepare dataset and ONNX model<a class="headerlink" href="#step-2-prepare-dataset-and-onnx-model" title="Permalink to this heading">#</a></h2>
<p>In this example, we utilize a custom ResNet model finetuned using the CIFAR-10 dataset</p>
<p>The <code class="docutils literal notranslate"><span class="pre">prepare_model_data.py</span></code> script downloads the CIFAR-10 dataset in pickle format (for python) and binary format (for C++). This dataset will be used in the subsequent steps for quantization and inference. The script also exports the provided PyTorch model into ONNX format. The following snippet from the script shows how the ONNX model is exported:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dummy_inputs = torch.randn(1, 3, 32, 32)
input_names = [&#39;input&#39;]
output_names = [&#39;output&#39;]
dynamic_axes = {&#39;input&#39;: {0: &#39;batch_size&#39;}, &#39;output&#39;: {0: &#39;batch_size&#39;}}
tmp_model_path = str(models_dir / &quot;resnet_trained_for_cifar10.onnx&quot;)
torch.onnx.export(
        model,
        dummy_inputs,
        tmp_model_path,
        export_params=True,
        opset_version=13,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
    )
</pre></div>
</div>
<p>Note the following settings for the onnx conversion:</p>
<ul class="simple">
<li><p>Ryzen AI supports a batch size=1, so dummy input is fixed to a batch_size =1 during model conversion</p></li>
<li><p>Recommended <code class="docutils literal notranslate"><span class="pre">opset_version</span></code> setting 13 is used.</p></li>
</ul>
<p>Run the following command to prepare the dataset and export the ONNX model:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python prepare_model_data.py
</pre></div>
</div>
<ul class="simple">
<li><p>The downloaded CIFAR-10 dataset is saved in the current directory at the following location: <code class="docutils literal notranslate"><span class="pre">data/*</span></code>.</p></li>
<li><p>The ONNX model is generated at models/resnet_trained_for_cifar10.onnx</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="step-3-quantize-the-model">
<h2>Step 3: Quantize the Model<a class="headerlink" href="#step-3-quantize-the-model" title="Permalink to this heading">#</a></h2>
<p>Quantizing AI models from floating-point to 8-bit integers reduces computational power and the memory footprint required for inference. This example utilizes Quark for ONNX quantizer workflow. Quark takes the pre-trained float32 model from the previous step (<code class="docutils literal notranslate"><span class="pre">resnet_trained_for_cifar10.onnx</span></code>) and provides a quantized model.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python resnet_quantize.py
</pre></div>
</div>
<p>This generates a quantized model using QDQ quant format and generate Quantized model with default configuration. After the completion of the run, the quantized ONNX model <code class="docutils literal notranslate"><span class="pre">resnet_quantized.onnx</span></code> is saved to models/resnet_quantized.onnx</p>
<p>The <code class="file docutils literal notranslate"><span class="pre">resnet_quantize.py</span></code> file has <code class="docutils literal notranslate"><span class="pre">ModelQuantizer::quantize_model</span></code> function that applies quantization to the model.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from quark.onnx.quantization.config import (Config, get_default_config)
from quark.onnx import ModelQuantizer

# Get quantization configuration
quant_config = get_default_config(&quot;XINT8&quot;)
config = Config(global_quant_config=quant_config)

# Create an ONNX quantizer
quantizer = ModelQuantizer(config)

# Quantize the ONNX model
quantizer.quantize_model(input_model_path, output_model_path, dr)
</pre></div>
</div>
<p>The parameters of this function are:</p>
<ul class="simple">
<li><p><strong>input_model_path</strong>: (String) The file path of the model to be quantized.</p></li>
<li><p><strong>output_model_path</strong>: (String) The file path where the quantized model is saved.</p></li>
<li><p><strong>dr</strong>: (Object or None) Calibration data reader that enumerates the calibration data and producing inputs for the original model. In this example, CIFAR10 dataset is used for calibration during the quantization process.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="step-4-deploy-the-model">
<h2>Step 4: Deploy the Model<a class="headerlink" href="#step-4-deploy-the-model" title="Permalink to this heading">#</a></h2>
<p>We demonstrate deploying the quantized model using both Python and C++ APIs.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#dep-python"><span class="std std-ref">Deployment - Python</span></a></p></li>
<li><p><a class="reference internal" href="#dep-cpp"><span class="std std-ref">Deployment - C++</span></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During the Python and C++ deployment, the compiled model artifacts are saved in the cache folder named <code class="docutils literal notranslate"><span class="pre">&lt;run</span> <span class="pre">directory&gt;/modelcachekey</span></code>. Ryzen-AI does not support the complied model artifacts across the versions, so if the model artifacts exist from the previous software version, ensure to delete the folder <code class="docutils literal notranslate"><span class="pre">modelcachekey</span></code> before the deployment steps.</p>
</div>
<section id="deployment-python">
<span id="dep-python"></span><h3>Deployment - Python<a class="headerlink" href="#deployment-python" title="Permalink to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">predict.py</span></code> script is used to deploy the model. It extracts the first ten images from the CIFAR-10 test dataset and converts them to the .png format. The script then reads all those ten images and classifies them by running the quantized custom ResNet model on CPU or NPU.</p>
<section id="deploy-the-model-on-the-cpu">
<h4>Deploy the Model on the CPU<a class="headerlink" href="#deploy-the-model-on-the-cpu" title="Permalink to this heading">#</a></h4>
<p>By default, <code class="docutils literal notranslate"><span class="pre">predict.py</span></code> runs the model on CPU.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python predict.py
</pre></div>
</div>
<p>Typical output</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Image 0: Actual Label cat, Predicted Label cat
Image 1: Actual Label ship, Predicted Label ship
Image 2: Actual Label ship, Predicted Label airplane
Image 3: Actual Label airplane, Predicted Label airplane
Image 4: Actual Label frog, Predicted Label frog
Image 5: Actual Label frog, Predicted Label frog
Image 6: Actual Label automobile, Predicted Label automobile
Image 7: Actual Label frog, Predicted Label frog
Image 8: Actual Label cat, Predicted Label cat
Image 9: Actual Label automobile, Predicted Label automobile
</pre></div>
</div>
</section>
<section id="deploy-the-model-on-the-ryzen-ai-npu">
<h4>Deploy the Model on the Ryzen AI NPU<a class="headerlink" href="#deploy-the-model-on-the-ryzen-ai-npu" title="Permalink to this heading">#</a></h4>
<p>To successfully run the model on the NPU, run the following setup steps:</p>
<ul class="simple">
<li><p>Make sure to set the XLNX_VART_FIRMWARE environment variable based to your APU type. Refer to <a class="reference internal" href="runtime_setup.html#npu-configurations"><span class="std std-ref">runtime setup instructions</span></a> on how to do this.</p></li>
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> points to <code class="docutils literal notranslate"><span class="pre">path\to\ryzen-ai-sw-&lt;version&gt;\</span></code>. If you installed Ryzen-AI software using the MSI installer, this variable should already be set. Ensure that the Ryzen-AI software package has not been moved post installation, in which case <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> will have to be set again.</p></li>
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">vaip_config.json</span></code> runtime configuration file from the installation package to the current directory. The <code class="docutils literal notranslate"><span class="pre">vaip_config.json</span></code> is used by the source file <code class="docutils literal notranslate"><span class="pre">predict.py</span></code> to configure the Vitis AI Execution Provider.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xcopy<span class="w"> </span>%RYZEN_AI_INSTALLATION_PATH%<span class="se">\v</span>oe-4.0-win_amd64<span class="se">\v</span>aip_config.json<span class="w"> </span>.
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>parser = argparse.ArgumentParser()
parser.add_argument(&#39;--ep&#39;, type=str, default =&#39;cpu&#39;,choices = [&#39;cpu&#39;,&#39;npu&#39;], help=&#39;EP backend selection&#39;)
opt = parser.parse_args()

providers = [&#39;CPUExecutionProvider&#39;]
provider_options = [{}]

if opt.ep == &#39;npu&#39;:
   providers = [&#39;VitisAIExecutionProvider&#39;]
   cache_dir = Path(__file__).parent.resolve()
   provider_options = [{
              &#39;config_file&#39;: &#39;vaip_config.json&#39;,
              &#39;cacheDir&#39;: str(cache_dir),
              &#39;cacheKey&#39;: &#39;modelcachekey&#39;
              }]

session = ort.InferenceSession(model.SerializeToString(), providers=providers,
                               provider_options=provider_options)
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">predict.py</span></code> with the <code class="docutils literal notranslate"><span class="pre">--ep</span> <span class="pre">npu</span></code> switch to run the custom ResNet model on the Ryzen AI NPU:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python predict.py --ep npu
</pre></div>
</div>
<p>Typical output</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[Vitis AI EP] No. of Operators :   CPU     2    IPU   398  99.50%
[Vitis AI EP] No. of Subgraphs :   CPU     1    IPU     1 Actually running on IPU     1
...
Image 0: Actual Label cat, Predicted Label cat
Image 1: Actual Label ship, Predicted Label ship
Image 2: Actual Label ship, Predicted Label ship
Image 3: Actual Label airplane, Predicted Label airplane
Image 4: Actual Label frog, Predicted Label frog
Image 5: Actual Label frog, Predicted Label frog
Image 6: Actual Label automobile, Predicted Label truck
Image 7: Actual Label frog, Predicted Label frog
Image 8: Actual Label cat, Predicted Label cat
Image 9: Actual Label automobile, Predicted Label automobile
</pre></div>
</div>
</section>
</section>
<section id="deployment-c">
<span id="dep-cpp"></span><h3>Deployment - C++<a class="headerlink" href="#deployment-c" title="Permalink to this heading">#</a></h3>
<section id="prerequisites">
<h4>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>Visual Studio 2022 Community edition, ensure “Desktop Development with C++” is installed</p></li>
<li><p>cmake (version &gt;= 3.26)</p></li>
<li><p>opencv (version=4.6.0) required for the custom resnet example</p></li>
</ol>
</section>
<section id="install-opencv">
<h4>Install OpenCV<a class="headerlink" href="#install-opencv" title="Permalink to this heading">#</a></h4>
<p>It is recommended to build OpenCV from the source code and use static build. The default installation localtion is “install” , the following instruction installs OpenCV in the location “C:\opencv” as an example. You may first change the directory to where you want to clone the OpenCV repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/opencv/opencv.git<span class="w"> </span>-b<span class="w"> </span><span class="m">4</span>.6.0
<span class="nb">cd</span><span class="w"> </span>opencv
cmake<span class="w"> </span>-DCMAKE_EXPORT_COMPILE_COMMANDS<span class="o">=</span>ON<span class="w"> </span>-DBUILD_SHARED_LIBS<span class="o">=</span>OFF<span class="w"> </span>-DCMAKE_POSITION_INDEPENDENT_CODE<span class="o">=</span>ON<span class="w"> </span>-DCMAKE_CONFIGURATION_TYPES<span class="o">=</span>Release<span class="w"> </span>-A<span class="w"> </span>x64<span class="w"> </span>-T<span class="w"> </span><span class="nv">host</span><span class="o">=</span>x64<span class="w"> </span>-G<span class="w"> </span><span class="s2">&quot;Visual Studio 17 2022&quot;</span><span class="w"> </span><span class="s2">&quot;-DCMAKE_INSTALL_PREFIX=C:\opencv&quot;</span><span class="w"> </span><span class="s2">&quot;-DCMAKE_PREFIX_PATH=C:\opencv&quot;</span><span class="w"> </span>-DCMAKE_BUILD_TYPE<span class="o">=</span>Release<span class="w"> </span>-DBUILD_opencv_python2<span class="o">=</span>OFF<span class="w"> </span>-DBUILD_opencv_python3<span class="o">=</span>OFF<span class="w"> </span>-DBUILD_WITH_STATIC_CRT<span class="o">=</span>OFF<span class="w"> </span>-B<span class="w"> </span>build
cmake<span class="w"> </span>--build<span class="w"> </span>build<span class="w"> </span>--config<span class="w"> </span>Release
cmake<span class="w"> </span>--install<span class="w"> </span>build<span class="w"> </span>--config<span class="w"> </span>Release
</pre></div>
</div>
<p>The build files will be written to <code class="docutils literal notranslate"><span class="pre">build\</span></code>.</p>
</section>
<section id="build-and-run-custom-resnet-c-sample">
<h4>Build and Run Custom Resnet C++ sample<a class="headerlink" href="#build-and-run-custom-resnet-c-sample" title="Permalink to this heading">#</a></h4>
<p>The C++ source files, CMake list files and related artifacts are provided in the <code class="docutils literal notranslate"><span class="pre">cpp/resnet_cifar/*</span></code> folder. The source file <code class="docutils literal notranslate"><span class="pre">cpp/resnet_cifar/resnet_cifar.cpp</span></code> takes 10 images from the CIFAR-10 test set, converts them to .png format, preprocesses them, and performs model inference. The example has onnxruntime dependencies, that are provided in <code class="docutils literal notranslate"><span class="pre">%RYZEN_AI_INSTALLATION_PATH%/onnxruntime/*</span></code>.</p>
<p>Run the following command to build the resnet example. Assign <code class="docutils literal notranslate"><span class="pre">-DOpenCV_DIR</span></code> to the OpenCV build directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>getting_started_resnet/cpp
cmake<span class="w"> </span>-DCMAKE_EXPORT_COMPILE_COMMANDS<span class="o">=</span>ON<span class="w"> </span>-DBUILD_SHARED_LIBS<span class="o">=</span>OFF<span class="w"> </span>-DCMAKE_POSITION_INDEPENDENT_CODE<span class="o">=</span>ON<span class="w"> </span>-DCMAKE_CONFIGURATION_TYPES<span class="o">=</span>Release<span class="w"> </span>-A<span class="w"> </span>x64<span class="w"> </span>-T<span class="w"> </span><span class="nv">host</span><span class="o">=</span>x64<span class="w"> </span>-DCMAKE_INSTALL_PREFIX<span class="o">=</span>.<span class="w"> </span>-DCMAKE_PREFIX_PATH<span class="o">=</span>.<span class="w"> </span>-B<span class="w"> </span>build<span class="w"> </span>-S<span class="w"> </span>resnet_cifar<span class="w"> </span>-DOpenCV_DIR<span class="o">=</span><span class="s2">&quot;C:/opencv/build&quot;</span><span class="w"> </span>-G<span class="w"> </span><span class="s2">&quot;Visual Studio 17 2022&quot;</span>
</pre></div>
</div>
<p>This should generate the build directory with the <code class="docutils literal notranslate"><span class="pre">resnet_cifar.sln</span></code> solution file along with other project files. Open the solution file using Visual Studio 2022 and build to compile. You can also use “Developer Command Prompt for VS 2022” to open the solution file in Visual Studio.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>devenv<span class="w"> </span>build/resnet_cifar.sln
</pre></div>
</div>
<p>Now to deploy our model, we will go back to the parent directory (getting_started_resnet) of this example. After compilation, the executable should be generated in <code class="docutils literal notranslate"><span class="pre">cpp/build/Release/resnet_cifar.exe</span></code>. We will copy this application over to the parent directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>..
xcopy<span class="w"> </span>cpp<span class="se">\b</span>uild<span class="se">\R</span>elease<span class="se">\r</span>esnet_cifar.exe<span class="w"> </span>.
</pre></div>
</div>
<p>Additionally, we will also need to copy the onnxruntime DLLs from the Vitis AI Execution Provider package to the current directory. The following commands copy the required files in the current directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xcopy<span class="w"> </span>%RYZEN_AI_INSTALLATION_PATH%<span class="se">\o</span>nnxruntime<span class="se">\b</span>in<span class="se">\*</span><span class="w"> </span>/E<span class="w"> </span>/I
</pre></div>
</div>
<p>The C++ application that was generated takes 3 arguments:</p>
<ol class="arabic simple">
<li><p>Path to the quantized ONNX model generated in Step 3</p></li>
<li><p>The execution provider of choice (cpu or NPU)</p></li>
<li><p>vaip_config.json (pass None if running on CPU)</p></li>
</ol>
<section id="id1">
<h5>Deploy the Model on the CPU<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h5>
<p>To run the model on the CPU, use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>resnet_cifar.exe<span class="w"> </span>models<span class="se">\r</span>esnet.qdq.U8S8.onnx<span class="w"> </span>cpu<span class="w"> </span>None
</pre></div>
</div>
<p>Typical output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>model<span class="w"> </span>name:models<span class="se">\r</span>esnet.qdq.U8S8.onnx
ep:cpu
Input<span class="w"> </span>Node<span class="w"> </span>Name/Shape<span class="w"> </span><span class="o">(</span><span class="m">1</span><span class="o">)</span>:
<span class="w">        </span>input<span class="w"> </span>:<span class="w"> </span>-1x3x32x32
Output<span class="w"> </span>Node<span class="w"> </span>Name/Shape<span class="w"> </span><span class="o">(</span><span class="m">1</span><span class="o">)</span>:
<span class="w">        </span>output<span class="w"> </span>:<span class="w"> </span>-1x10
Final<span class="w"> </span>results:
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>ship
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>airplane<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>airplane
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>truck<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>automobile
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>frog
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>cat
Predicted<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>automobile<span class="w"> </span>and<span class="w"> </span>actual<span class="w"> </span>label<span class="w"> </span>is<span class="w"> </span>automobile
</pre></div>
</div>
</section>
<section id="deploy-the-model-on-the-npu">
<h5>Deploy the Model on the NPU<a class="headerlink" href="#deploy-the-model-on-the-npu" title="Permalink to this heading">#</a></h5>
<p>To successfully run the model on the NPU:</p>
<ul class="simple">
<li><p>Make sure to set the XLNX_VART_FIRMWARE environment variable based to your APU type. Refer to <a class="reference internal" href="runtime_setup.html#npu-configurations"><span class="std std-ref">runtime setup instructions</span></a> on how to do this.</p></li>
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> points to <code class="docutils literal notranslate"><span class="pre">path\to\ryzen-ai-sw-&lt;version&gt;\</span></code>. If you installed Ryzen-AI software using the MSI installer, this variable should already be set. Ensure that the Ryzen-AI software package has not been moved post installation, in which case <code class="docutils literal notranslate"><span class="pre">RYZEN_AI_INSTALLATION_PATH</span></code> will have to be set again.</p></li>
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">vaip_config.json</span></code> runtime configuration file from the installation package to the current directory. The <code class="docutils literal notranslate"><span class="pre">vaip_config.json</span></code> is used by the source file <code class="docutils literal notranslate"><span class="pre">resnet_cifar.cpp</span></code> to configure the Vitis AI Execution Provider.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xcopy<span class="w"> </span>%RYZEN_AI_INSTALLATION_PATH%<span class="se">\v</span>oe-4.0-win_amd64<span class="se">\v</span>aip_config.json<span class="w"> </span>.
</pre></div>
</div>
<p>The following code block from <code class="docutils literal notranslate"><span class="pre">reset_cifar.cpp</span></code> shows how ONNX Runtime is configured to deploy the model on the Ryzen AI NPU:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>auto<span class="w"> </span><span class="nv">session_options</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>Ort::SessionOptions<span class="o">()</span><span class="p">;</span>

auto<span class="w"> </span><span class="nv">config_key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>std::string<span class="o">{</span><span class="w"> </span><span class="s2">&quot;config_file&quot;</span><span class="w"> </span><span class="o">}</span><span class="p">;</span>
auto<span class="w"> </span><span class="nv">cache_dir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>std::filesystem::current_path<span class="o">()</span>.string<span class="o">()</span><span class="p">;</span>

<span class="k">if</span><span class="o">(</span><span class="nv">ep</span><span class="o">==</span><span class="s2">&quot;npu&quot;</span><span class="o">)</span>
<span class="o">{</span>
auto<span class="w"> </span><span class="nv">options</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span>std::unordered_map&lt;std::string,<span class="w"> </span>std::string&gt;<span class="o">{</span><span class="w"> </span><span class="o">{</span>config_key,<span class="w"> </span>json_config<span class="o">}</span>,<span class="w"> </span><span class="o">{</span><span class="s2">&quot;cacheDir&quot;</span>,<span class="w"> </span>cache_dir<span class="o">}</span>,<span class="w"> </span><span class="o">{</span><span class="s2">&quot;cacheKey&quot;</span>,<span class="w"> </span><span class="s2">&quot;modelcachekey&quot;</span><span class="o">}</span><span class="w"> </span><span class="o">}</span><span class="p">;</span>
session_options.AppendExecutionProvider_VitisAI<span class="o">(</span>options<span class="o">)</span>
<span class="o">}</span>

auto<span class="w"> </span><span class="nv">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>Ort::Session<span class="o">(</span>env,<span class="w"> </span>model_name.data<span class="o">()</span>,<span class="w"> </span>session_options<span class="o">)</span><span class="p">;</span>
</pre></div>
</div>
<p>To run the model on the NPU, we will pass the npu flag and the vaip_config.json file as arguments to the C++ application. Use the following command to run the model on the NPU:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>resnet_cifar.exe<span class="w"> </span>models<span class="se">\r</span>esnet.qdq.U8S8.onnx<span class="w"> </span>npu<span class="w"> </span>vaip_config.json
</pre></div>
</div>
<p>Typical output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[Vitis AI EP] No. of Operators :   CPU     2    IPU   398  99.50%
[Vitis AI EP] No. of Subgraphs :   CPU     1    IPU     1 Actually running on IPU     1
...
Final results:
Predicted label is cat and actual label is cat
Predicted label is ship and actual label is ship
Predicted label is ship and actual label is ship
Predicted label is airplane and actual label is airplane
Predicted label is frog and actual label is frog
Predicted label is frog and actual label is frog
Predicted label is truck and actual label is automobile
Predicted label is frog and actual label is frog
Predicted label is cat and actual label is cat
Predicted label is automobile and actual label is automobile
</pre></div>
</div>
</section>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-install-packages">Step 1: Install Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-dataset-and-onnx-model">Step 2: Prepare dataset and ONNX model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-quantize-the-model">Step 3: Quantize the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-deploy-the-model">Step 4: Deploy the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-python">Deployment - Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-cpu">Deploy the Model on the CPU</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-ryzen-ai-npu">Deploy the Model on the Ryzen AI NPU</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment-c">Deployment - C++</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#install-opencv">Install OpenCV</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-run-custom-resnet-c-sample">Build and Run Custom Resnet C++ sample</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deploy the Model on the CPU</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-model-on-the-npu">Deploy the Model on the NPU</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on July 29, 2024.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        
                        <li><a href="">Ryzen AI Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2023 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>